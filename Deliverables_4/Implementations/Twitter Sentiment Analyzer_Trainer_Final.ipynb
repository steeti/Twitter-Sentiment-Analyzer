{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analyzer: Module Trainer Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mmvc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mmvc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import twitter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re as charRemoval\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import time\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the training data set and loading it into a panda data frame\n",
    "filename= \"trainingdataset17k.csv\"\n",
    "dataFramTraining = pd.read_csv(\"./Data/\"+filename, header=None, usecols=[0,5], names=['tweet polarity','text'], encoding=\"ISO-8859-1\")\n",
    "#grouping the data by labels\n",
    "dataFramTrainingNegative = dataFramTraining.loc[dataFramTraining['tweet polarity']==0]\n",
    "dataFramTrainingPositive = dataFramTraining.loc[dataFramTraining['tweet polarity']==4]\n",
    "SplittedTrainSet = [dataFramTrainingNegative.iloc[0:8500,:],dataFramTrainingPositive.iloc[0:8500,:]]\n",
    "#merging the two data sets\n",
    "MergedArrays = pd.concat(SplittedTrainSet)\n",
    "#defining polarity and labels\n",
    "polarity = [ (MergedArrays['tweet polarity'] == 0), (MergedArrays['tweet polarity'] == 4)]\n",
    "negative_positive = ['negative',  'positive']\n",
    "#replacing polarity numberss with labels\n",
    "MergedArrays['label'] = np.select(polarity, negative_positive)\n",
    "#converting the pandas datafram to a dictionary\n",
    "trainingset = MergedArrays.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to clean the tweets\n",
    "class TweetsPreProcessing:\n",
    "    def __init__(self):\n",
    "        # stop words are words that dont add much meaning to the sentence in the senitment\n",
    "        # and punctiotation also don't add much meaning to the sentence in the senitment\n",
    "        self.notimportantWords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL']) \n",
    "    #loops inside the tweet list and call the tweet cleaner function\n",
    "    def TweetsProcessing(self, tweetlists):\n",
    "        Tweets_Processed=[]\n",
    "        for t in tweetlists:\n",
    "            Tweets_Processed.append((self.TweetCleaner(t[\"text\"]),t[\"label\"]))  \n",
    "        return Tweets_Processed\n",
    "    # tweet cleaner function convert texts to lowercases and removes hashtags urls and usernames\n",
    "    def TweetCleaner(self, t):\n",
    "        urlLinks='((www\\.[^\\s]+)|(https?://[^\\s]+))'\n",
    "        username='@[^\\s]+'\n",
    "        hashtags= r'#([^\\s]+)'\n",
    "        t = t.lower()\n",
    "        t = charRemoval.sub(urlLinks, 'URL', t) \n",
    "        t = charRemoval.sub(username, 'AT_USER', t)\n",
    "        t = charRemoval.sub(hashtags, r'\\1', t) \n",
    "        t = word_tokenize(t)\n",
    "        wordcleaned=[]\n",
    "        #loop through t and if the word is important then append it\n",
    "        for w in t:\n",
    "            if w not in self.notimportantWords:\n",
    "                wordcleaned.append(w)\n",
    "        return wordcleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLTKProcessor1(Training_Data_Pre):\n",
    "    #nltk proccessor that builds vocabulary set\n",
    "    vocab = []\n",
    "    #for loop in the training data set and extend i\n",
    "    for (i, j) in Training_Data_Pre:\n",
    "        vocab.extend(i)\n",
    "    #return those frequencies of vocab as keys.\n",
    "    return nltk.FreqDist(vocab).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLTKProcessor2(tweet):\n",
    "    #nltk proccessor that extract features set\n",
    "    WordtweetFeatures=set(tweet)\n",
    "    f={}\n",
    "    for i in buildedvocab:\n",
    "            #if word in the vocab is in the tweet and returns true false\n",
    "            # using JSON key\n",
    "        f['contains(%s)' % i]=(i in WordtweetFeatures)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preproccessing the tweets in trainset\n",
    "TrainSet_PreProccesing = TweetsPreProcessing().TweetsProcessing(trainingset)\n",
    "#building vocabulary\n",
    "buildedvocab = NLTKProcessor1(TrainSet_PreProccesing)\n",
    "#Make a training feature vector using NLTKProccessor1 and NLTKProccessor2\n",
    "features_training=nltk.classify.apply_features(NLTKProcessor2,TrainSet_PreProccesing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with file:  trainingdataset17k.csv\n",
      "Building Training Module Accomplished\n",
      "CPU times: user 2min 49s, sys: 9.09 s, total: 2min 58s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train the module using BernoulliNB\n",
    "print(\"Training with file: \",filename)\n",
    "SKLearnClassifier = SklearnClassifier(BernoulliNB()).train(features_training)\n",
    "print(\"Building Training Module Accomplished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the module to use in other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the module data\n",
    "save_classifier = open(\"TrainModuleClassifierNLP.pickle\",\"wb\")\n",
    "pickle.dump(SKLearnClassifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
